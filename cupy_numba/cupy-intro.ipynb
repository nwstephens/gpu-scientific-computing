{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4941058a-6c14-4330-8dee-a9e137c516f3",
   "metadata": {},
   "source": [
    "## **Notebook 1: Introduction to CuPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb079367-934a-4d8e-a5c3-ec5592ac69ae",
   "metadata": {},
   "source": [
    "**Introduction to Workshop Lab Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe05310-7998-4a4f-a660-f762f3359fab",
   "metadata": {},
   "source": [
    "Markdown cells contain plain text, and code cells contain interactive Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06d849-08b0-4dfc-b76d-b60be5b172cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f80bd-c004-4d16-9f1b-974c971d2235",
   "metadata": {},
   "source": [
    "We can also run shell commands by prepending an exclamation mark to our code cells. Let's query for some basic information about our system. `nvidia-smi` is like `top` for NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74837c5-d1ed-4a7f-ba8c-a0d99c668093",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fd29b-e458-4c99-ab63-cb456524d5e3",
   "metadata": {},
   "source": [
    "Let's check out the connection topology of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03404cd-d681-46cd-b91a-1d1784eefcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c68ba-47c7-4f75-ba06-2244fc1ce936",
   "metadata": {},
   "source": [
    "And finally, let's check out the type of CPU we were allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce24e3-22d0-4cd8-9691-132259ef2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a5376-6bd0-4d23-b61a-7a2954abab6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Introduction to CuPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbcc303-3c50-45c6-82f1-e7068e6d0665",
   "metadata": {},
   "source": [
    "NumPy is a widely used library for numerical computing in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf823dc-3fef-431f-9abb-ee827c0cf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = 512\n",
    "\n",
    "A = np.random.randn(size, size)\n",
    "\n",
    "%timeit -n 5 Q, R = np.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7dcb8c-7141-46f3-bd08-ea07261e9e65",
   "metadata": {},
   "source": [
    "CuPy uses a NumPy-like interface. Porting a Numpy code to CuPy can be as simple as changing your import statement. In this workshop, we'll always use `import cupy as cp` for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d196d0-c618-4805-863a-3b57e873d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "size = 512\n",
    "\n",
    "A = cp.random.randn(size, size)\n",
    "\n",
    "Q, R = cp.linalg.qr(A)\n",
    "%timeit -n 5 Q, R = cp.linalg.qr(A) ; cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954a9de-3519-400e-8a44-d59668bd26f2",
   "metadata": {},
   "source": [
    "We already see a substantial speedup with no real code changes! \n",
    "\n",
    "Notice the additional call to `cp.cuda.Device().synchronize()` in the CuPy version. GPU kernel calls are asynchronous with respect to the CPU. Our call to `synchronize()` ensures the GPU finishes to completion, so we can accurately measure  the elapsed time. We don't generally need to add these calls to production CuPy codes.\n",
    "\n",
    "NumPy is typically used to perform computations on _arrays_ of data. The data is stored in the `numpy.ndarray` object. CuPy implements a similar class called the `cupy.ndarray`. But while the `numpy.ndarray` data resides in host memory, the contents of a `cupy.ndarray` persistent in GPU memory. CuPy provides several helper functions to convert between Cupy and NumPy `ndarrays` - facilitating data transfer to/from the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4b14c-fe6e-4ba7-82fc-0c50219e1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the data on the host\n",
    "A_cpu = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
    "\n",
    "print(\"A_cpu is a\", type(A_cpu))\n",
    "print(\"With initial values:\\n\", A_cpu)\n",
    "\n",
    "#Copy data, host to device\n",
    "A_gpu = cp.asarray(A_cpu)\n",
    "print(\"A_gpu is a\", type(A_gpu))\n",
    "\n",
    "#Square the data on the device\n",
    "A_gpu = cp.square(A_gpu)\n",
    "\n",
    "#Copy data, device to host\n",
    "A_cpu = cp.asnumpy(A_gpu)\n",
    "\n",
    "print(\"Squared values:\\n\", A_cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570d8dd-7f54-4d65-83b7-6cc38d6b4cc2",
   "metadata": {},
   "source": [
    "Note that NumPy and CuPy ndarrys are not implicitly convertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eacf74-5f07-42b0-9b6f-8af7a302d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.square(A_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ba905-52c8-44af-9bea-461cd63e13c4",
   "metadata": {},
   "source": [
    "CuPy is useful for programming multi-GPU nodes as well. We can orchestrate computation, data movement, and other low-level CUDA operations with functions in the `cupy.cuda` namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968a53d-3a98-4c2b-92b8-a780f5949d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize array on GPU 1\n",
    "with cp.cuda.Device(1):\n",
    "    A_gpu_1 = cp.array([[1, 2, 3], [4, 5, 6]], cp.int32)\n",
    "\n",
    "#Copy array from GPU 1 to GPU 0\n",
    "A_gpu_0 = cp.asarray(A_gpu_1)\n",
    "\n",
    "print(A_gpu_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84591b-2329-44a8-a266-baa30866086b",
   "metadata": {},
   "source": [
    "The GPU is a powerhouse of parallel computing performance, and can process math operations much more quickly than the CPU. This is easy to see by comparing performance of CuPy vs NumPy, particularly for dense linear algebra operations. Let's look at a multiplication of 4096x4096 matrices. Notice the similarity of the two versions of the code (NumPy and CuPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64595b-a9b7-43c4-a5ed-87b579aca4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from time import perf_counter\n",
    "\n",
    "size = 4096\n",
    "\n",
    "start_time = perf_counter( )\n",
    "A_cpu = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n",
    "B_cpu = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n",
    "C_cpu = np.matmul(A_cpu,B_cpu)\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A_cpu\n",
    "del B_cpu\n",
    "del C_cpu\n",
    "\n",
    "\n",
    "\n",
    "A_gpu = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n",
    "B_gpu = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu) #Exclude one-time JIT overhead\n",
    "start_time = perf_counter( )\n",
    "C_gpu = cp.matmul(A_gpu,B_gpu)\n",
    "cp.cuda.Device(0).synchronize()\n",
    "stop_time = perf_counter( )\n",
    "\n",
    "print('')\n",
    "print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n",
    "print('')\n",
    "\n",
    "del A_gpu\n",
    "del B_gpu\n",
    "del C_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9b000-e8fa-4f67-a219-4640aea54abf",
   "metadata": {},
   "source": [
    "The GPU's strenghts in computational throughput and memory bandwidth can lead to terrific application speedups. But we need to be considerate of two types of overhead when evaluating our problem for acceleration on the GPU with CuPy: kernel overhead, and data movement overhead.\n",
    "\n",
    "---\n",
    "\n",
    "**Kernel Overhead**\n",
    "\n",
    "CuPy compiles kernel codes on-the-fly using JIT compilation. Therefore, there is a compilation overhead the first time a given function is called with CuPy. The compiled kernel code is cached, so compilation overhead is avoided for subsequent executions of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a0d2b-da82-4e46-999c-55bbad4c0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "size = 512\n",
    "for _ in range(5):\n",
    "    A = cp.random.randn(size, size).astype(np.float32)\n",
    "    t1 = time.time()\n",
    "    cp.linalg.det(A)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    t2 = time.time()\n",
    "    print('%.4f' % (t2 - t1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d8d9b-e4b2-4376-a441-b6bbf9a78af8",
   "metadata": {},
   "source": [
    "You may also notice a one-time overhead upon first calling a CuPy function in a program. This overhead is associated with the creation of a CUDA context by the CUDA driver, which happens the first time any CUDA API is invoked in a program.\n",
    "\n",
    "In addition, there is a CUDA kernel launch overhead that is penalized each time a GPU kernel is launched. The overhead is on the order of a few microseconds. For this reason, launching many small CUDA kernels in an application will generally lead to poor performance. The kernel launch overhead may dominate your runtime for very small problems, but for large datasets the overhead will be small compared to the actual GPU computation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2061b6b0-169f-4e64-8556-b3fd86b97dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in [64, 128, 256, 512, 1024, 2048]:\n",
    "    print(\"\\nInput Matrix size: %d\" % size, \"x %d \" % size)\n",
    "    for xp in [np, cp]:\n",
    "        A=xp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(xp.float32)\n",
    "        xp.linalg.qr(A)#Exclude potential one-time JIT overhead\n",
    "        t1 = time.time()\n",
    "        xp.linalg.qr(A)\n",
    "        cp.cuda.Device().synchronize()\n",
    "        t2 = time.time()\n",
    "        print(xp.__name__, '%f' % (t2 - t1))\n",
    "        del A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff335660-dfa6-4657-9489-7d31bb491d93",
   "metadata": {},
   "source": [
    "It's clear that increasing the problem size can help amoritize the overhead of launching GPU kernels. Another common strategy is to merge multiple kernels together into a single combined kernel, reducing the total number of kernel launches in your program. CuPy supports kernel fusion in this manner via the `@cupy.fuse()` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b1c62-62f6-4da9-a3de-3e5afd5beda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_diff(x, y):\n",
    "    return (x - y) * (x - y)\n",
    "\n",
    "@cp.fuse\n",
    "def fused_squared_diff(x, y):\n",
    "    return (x - y) * (x - y)\n",
    "\n",
    "size = 10000\n",
    "\n",
    "x = cp.arange(size)\n",
    "y = cp.arange(size)[::-1]\n",
    "\n",
    "%timeit -n 10 squared_diff(x, y); cp.cuda.Device().synchronize()\n",
    "%timeit -n 10 fused_squared_diff(x, y); cp.cuda.Device().synchronize()\n",
    "\n",
    "del x\n",
    "del y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e7785-3b82-4e6e-9742-76b1caed35c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Data Movement Overhead**\n",
    "\n",
    "Try to minimize data movement to or from the GPU. The FLOP rate and memory bandwidth of a GPU can process data much more quickly than it can be fed with data over the PCIe bus. This problem is being tackled with novel interconnect technologies like NVLink. But it's a real inbalance we have to deal with for now.\n",
    "Let's look at an example where we initialize our input data GPU and then computes the dot product. Note that the result of the multiplication, the C matrix, is available on the GPU in case we need it later.\n",
    "\n",
    "Notice again the similarity of the two parts of the code (NumPy and CuPy). They are virtually identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb0179-8137-4827-8eb2-2c960967593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    C_cpu = np.dot(A_cpu,B_cpu)\n",
    "    stop_time = perf_counter( )\n",
    "    cpu_time = stop_time - start_time\n",
    "    print('numpy = %g seconds' % cpu_time )\n",
    "\n",
    "    start_time = perf_counter( )\n",
    "    A_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    B_gpu=cp.random.rand(size).astype(cp.float32)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ea831-f9e3-41c1-888d-8fed77d9fd1f",
   "metadata": {},
   "source": [
    "But what if the input data for the `dot` operation resides in the system memory? We need to move the data over the PCIe bus (from the host to the GPU) using `cp.asarray()`. \n",
    "\n",
    "Modify the following cell to initialize the ndarray data with Numpy. \n",
    "\n",
    "How does the speedup change after the additional cost of data movement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be63e60-fb09-4936-886d-7e91aebacd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    start_time = perf_counter( )\n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "\n",
    "    start_time = perf_counter( )\n",
    "#>>>Insert CuPy code here\n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc692d-137a-44ab-8743-3a2fa3ff60bc",
   "metadata": {},
   "source": [
    "Click the `...` below to reveal the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a42a53-e98c-4434-95ab-2114ee4d6dcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = int(1e8)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Iteration \", i)\n",
    "    \n",
    "    start_time = perf_counter( )\n",
    "    \n",
    "    A_cpu=np.random.rand(size).astype(np.float32)\n",
    "    B_cpu=np.random.rand(size).astype(np.float32)\n",
    "    \n",
    "    A_gpu=cp.asarray(A_cpu)\n",
    "    B_gpu=cp.asarray(B_cpu)\n",
    "    C_gpu = cp.dot(A_gpu,B_gpu)\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    \n",
    "    stop_time = perf_counter( )\n",
    "    gpu_time = stop_time - start_time\n",
    "    \n",
    "    print('cupy = %g seconds' % gpu_time )\n",
    "    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e8fff-90ec-4af3-b453-d1a97d83c86f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Managing GPU Memory**\n",
    "\n",
    "Modern datacenter GPUs have as much as 80GB of high-bandwidth memory on a single accelerator. But in general, the host system memory will have a larger capacity. We need to be conscious of GPU memory limitations when transfering data from the host. We can query the amount of free and total memory with nvidia-smi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af7910-1168-4bc6-bc9a-f6e887f2bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -i 0 --query-gpu=memory.free,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18ee84-662a-4611-b142-0844ff46bb0b",
   "metadata": {},
   "source": [
    "Or natively with CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc8213-3cad-4d2b-a6dc-c345db8c1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU (free, total) memory in bytes:\")\n",
    "print(cp.cuda.Device().mem_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038935eb-bce2-406e-9a4e-a07b23764c08",
   "metadata": {},
   "source": [
    "Let's clear all GPU memory for good measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4d096-f36f-43eb-bdc7-7af50327342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "print(\"GPU (free, total) memory in bytes:\")\n",
    "print(cp.cuda.Device().mem_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659485d-679c-49f3-81ec-4be29295fe9d",
   "metadata": {},
   "source": [
    "What happens if we try to allocate too much space on the GPU? In the following example, arrays A and B are 8GB each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e011c0b-1b76-4320-bd09-9769379c543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 32768\n",
    "A = cp.ones((size, size))\n",
    "B = cp.ones((size, size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8d6e1-240c-4883-8462-4e1dab976ca5",
   "metadata": {},
   "source": [
    "One possible solution is to switch over to unified memory. With unified memory, the CUDA runtime will migrate data between the CPU and GPU _on demand_. Data migrations are triggered by page faults, so we may be leaving some performance on the table by using unified memory instead of managing memory explicitly. But it's an extremely convenient feature for making GPUs easier to program. We can enable Unified Memory in CuPy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c121f48-b01e-4b17-a83c-3210472ae678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a memory pool instance with malloc_managed allocator\n",
    "pool = cp.cuda.MemoryPool(cp.cuda.malloc_managed)\n",
    "cp.cuda.set_allocator(pool.malloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0a2ab-da68-49df-b712-a7cbba943c3b",
   "metadata": {},
   "source": [
    "Let's try that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ad44e-dcc0-475f-a085-b770dd105ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 32768\n",
    "A = cp.ones((size, size))\n",
    "B = cp.ones((size, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58382e8-2e73-488c-b318-6974deb80eba",
   "metadata": {},
   "source": [
    "We can certainly perform computations on these new arrays. Performance will take a hit as the GPU swaps pages in-and-out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b8daa-ef79-42f6-ac3a-7213f7fbb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.add(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82076ac4-4ec3-4913-a891-87f029760155",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Please restart the kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbe283-6890-4758-b8b6-9d5f6d3fdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
